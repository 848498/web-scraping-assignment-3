{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0koccMZKc9fL4pCJnPJ46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/848498/web-scraping-assignment-3/blob/main/web_scrping_assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ETr2lhXX7ipX",
        "outputId": "d0ad840e-c252-4a5f-f629-88d014b1e465"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-397e81fa3760>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    title = product.find('span', {'class': 'a-size-medium'}).text.strip()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 19\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def search_product(product):\n",
        "  # Format the search query\n",
        "  search_query = product.replace(' ', '+')\n",
        "\n",
        "  # Send a GET request to Amazon.in search page\n",
        "  url = f'https://www.amazon.in/s?k={search_query}'\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Parse the HTML content using BeautifulSoup\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  # Find all the product elements on the page\n",
        "  products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "  # Iterate over the products and extract relevant information\n",
        "  for product in products:\n",
        "  # Extract the product title\n",
        "  title = product.find('span', {'class': 'a-size-medium'}).text.strip()\n",
        "\n",
        "  # Extract the product price\n",
        "  price = product.find('span', {'class': 'a-price-whole'}).text.strip()\n",
        "\n",
        "  # Extract the product rating\n",
        "  rating = product.find('span', {'class': 'a-icon-alt'}).text.strip()\n",
        "\n",
        "  # Print the product information\n",
        "  print(f'Title: {title}')\n",
        "  print(f'Price: {price}')\n",
        "  print(f'Rating: {rating}')\n",
        "  print('---')\n",
        "\n",
        "# Take user input for the product to search\n",
        "product = input('Enter the product to search: ')\n",
        "\n",
        "# Call the search_product function with the user input\n",
        "search_product(product)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "\n",
        "# Set up the Chrome driver\n",
        "driver = webdriver.Chrome('path_to_chromedriver')\n",
        "\n",
        "# Open images.google.com\n",
        "driver.get('https://images.google.com')\n",
        "\n",
        "# Define the keywords\n",
        "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
        "\n",
        "# Scrape 10 images for each keyword\n",
        "for keyword in keywords:\n",
        "  # Find the search bar element and enter the keyword\n",
        "  search_bar = driver.find_element_by_name('q')\n",
        "  search_bar.clear()\n",
        "  search_bar.send_keys(keyword)\n",
        "\n",
        "  # Find the search button element and click it\n",
        "  search_button = driver.find_element_by_css_selector('button[type=\"submit\"]')\n",
        "  search_button.click()\n",
        "\n",
        "  # Wait for the page to load\n",
        "  time.sleep(2)\n",
        "\n",
        "  # Scroll down to load more images\n",
        "  driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "  time.sleep(2)\n",
        "\n",
        "  # Scrape the images\n",
        "  images = driver.find_elements_by_css_selector('img.rg_i')\n",
        "  for i in range(10):\n",
        "  image_url = images[i].get_attribute('src')\n",
        "  print(f\"Scraped image {i+1} for keyword '{keyword}': {image_url}\")\n",
        "\n",
        "  # Go back to the search page\n",
        "  driver.execute_script(\"window.history.go(-1)\")\n",
        "  time.sleep(2)\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "BvdIzTpEnGj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "url = f\"https://www.flipkart.com/search?q={search_query}&page=1\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "smartphones = []\n",
        "result = soup.find_all('div', {'class': '_1AtVbE'})\n",
        "brand = result.find('div', {'class': '_4rR01T'}).text.strip()\n",
        "name = result.find('a', {'class': 'IrpwTa'}).text.strip()\n",
        "url = \"https://www.flipkart.com\" + result.find('a', {'class': 'IRpwTa'})['href']\n",
        "color = result.find('a', {'class': '_1WMLwI'}).text.strip()\n",
        "color = \"_\"\n",
        "ram = result.find('ul', {'class': '_1xgFaF'}).find_all('li')[0].text.strip()\n",
        "ram = \"_\"\n",
        "storage = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[1].text.strip()\n",
        "storage = \"_\"\n",
        "primary_camera = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[2].text.strip()\n",
        "primary_camera = \"_\"\n",
        "secondary_camera = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[3].text.strip()\n",
        "secondary_camera = \"_\"\n",
        "display_size = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[4].text.strip()\n",
        "display_size = \"_\"\n",
        "battery_capacity = result.find('ul', {'class': '_1xgFaf'}).find_all('li')[5].text.strip()\n",
        "battery_capacity = \"_\"\n",
        "price = result.find('div', {'class': '_30jeq3 _1_WHN1'}).text.strip()\n",
        "price = \"_\"\n",
        "smartphone = {}\n",
        "\"Brand Name\":brand\n",
        "\"Smartphone Name\": name\n",
        "\"Colour\": color\n",
        "\"RAM\": ram\n",
        "\"Storage(ROM)\": storage\n",
        "\"Primary Camera\": primary_camera\n",
        "\"Secondary Camera\": secondary_camera\n",
        "\"Display Size\": display_size\n",
        "\"Battery Capacity\": battery_capacity\n",
        "\"Price\": price\n",
        "\"Product URL\": url\n",
        "{}\n",
        "smartphones.append(smartphones)\n",
        "df = pd.DataFrame(smartphones)\n",
        "df.to_csv('smartphones_csv', index=False)\n",
        "return df\n",
        "search_query = \"Oneplus Nord\"\n",
        "df = scrape_flipkart_smartphones(search_query)\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DNYK6NV3G1AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "query = f\"google maps\"{city} coordinates\"\n",
        "response = requests.get(f\"https://www.google.com/search?q{query}\")\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "result = soup.find(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
        "coordinates = result.text.split(\",\")\n",
        "latitude = coordinates[0].strip()\n",
        "longitude = coordinates[1].strip()\n",
        "return latitude, longitude\n",
        "city = \"New York\"\n",
        "latitude, longitude = get_coordinates(city)\n",
        "print(f\"The coordinates of {city}) Latitude - {latitude}, Longitude - {longitude}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rTgqaRfQaH5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.shape_base import column_stack\n",
        "from numpy.lib.shape_base import column_stack\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://www.forbes.com\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "table = soup.find(\"table\", class_=\"table\")\n",
        "rows = table.find_all(\"tr\")\n",
        "columns = row.find_all(\"td\")\n",
        "rank = columns[0].text.strip()\n",
        "name = columns[1].text.strip()\n",
        "net_worth = columns[2].text.strip()\n",
        "age = columns[3].text.strip()\n",
        "citizenship = columns[4].text.strip()\n",
        "source = columns[5].text.strip()\n",
        "industry = columns[6].text.strip()\n",
        "print(\"Rank:\", rank)\n",
        "print(\"Name:\", name)\n",
        "print(\"Net worth:\", net_worth)\n",
        "print(\"Age\", age)\n",
        "print(\"Citizenship:\", citizenship)\n",
        "print(\"Source:\", source)\n",
        "print(\"Industry:\", industry)\n",
        "print(\"--------------------\")\n"
      ],
      "metadata": {
        "id": "-d2waR0Ze4SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "driver = webdriver.Chrome('path_to_chromedriver')\n",
        "video_url = 'https://www.youtube.com/watch?v=your_video_id'\n",
        "driver.get(video_url)\n",
        "scroll_pause_time = 2\n",
        "scrolls = 10\n",
        "driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "  time.sleep(scroll_pause_time)\n",
        "\n",
        "comments = driver.find_elements_by_xpath('//yt-formatted-string[@id=\"content-text\"]')\n",
        "upvotes = driver.find_elements_by_xpath('//span[@id=\"vote-count-middle\"]')\n",
        "times = driver.find_elements_by_xpath('//a@class=\"yt-simple-endpoint sytle-scope yt-formatted-string\"]')\n",
        "extracted_data = []\n",
        "extracted_data.append({\n",
        "    'comment': comment.text,\n",
        "    'upvote': upvote.text,\n",
        "    'time': time.text\n",
        "})\n",
        "driver.quite()\n",
        "for data in extracted_data:\n",
        "  print(data)\n"
      ],
      "metadata": {
        "id": "tRpiIrNXndwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://www.hostelworld.com/hostel/London\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content,\"html.parser\")\n",
        "hostel_listing = soup.find_all(\"div\", class_=\"fabresult\")\n",
        "name = hostel.find(\"h2\", class_=\"title\").text.strip()\n",
        "distane = hostel.find(\"span\", class_=\"distance\").text.strip()\n",
        "ratings = hostel.find(\"div\", class_=\"score orange\").text.strip()\n",
        "total_reviews = hostel.find(\"div\", class_=\"reviews\").text.strip()\n",
        "overall_reviews = hostel.find(\"div\", class_=\"rating\").text.strip()\n",
        "private_price = hostel.find(\"div\", class_=\"price\").text.strip()\n",
        "dorms_price = hostel.find(\"div\", class_=\"price dorms\").text.strip()\n",
        "facilities = [facilities.text.strip()for facility in hostel.find_all(\"span\", class_=\"facilities\")]\n",
        "description = hostel.find(\"div\", class_=\"desc\").text.strip()\n",
        "print(\"Name:\", name)\n",
        "print(\"Distance from city centre:\", distance)\n",
        "print(\"Ratings:\", ratings)\n",
        "print(\"Total reviews:\", total_reviews)\n",
        "print(\"Overall reviews:\", overall_reviews)\n",
        "print(\"Privates from price:\", private_price)\n",
        "print(\"Dorms from price:\", dorms_price)\n",
        "print(\"Facilities:\", facilities)\n",
        "print(\"Description:\", description)\n",
        "print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HDqLNKPjwjw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}